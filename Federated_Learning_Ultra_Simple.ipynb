{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Ultra-Simple Federated Learning Demo\n",
    "\n",
    "**Zero dependency conflicts - works on any Google Colab**\n",
    "\n",
    "## ğŸ¯ What This Demonstrates\n",
    "- **Federated Learning**: Multiple clients training on distributed data\n",
    "- **Drift Detection**: Performance monitoring and anomaly detection\n",
    "- **Model Aggregation**: Combining client models into global model\n",
    "- **Recovery**: Adapting when drift is detected\n",
    "\n",
    "## ğŸ’¡ Key Features\n",
    "- Uses only PyTorch (pre-installed in Colab)\n",
    "- No external dependencies to conflict\n",
    "- Synthetic data generation (no download required)\n",
    "- Complete federated learning pipeline\n",
    "- Built-in visualization with matplotlib\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ Zero-Conflict Setup\n",
    "\n",
    "This notebook uses only libraries pre-installed in Google Colab to avoid any dependency conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import only pre-installed libraries (no pip installs needed!)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ® Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ“Š GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"âœ… Zero-dependency setup complete!\")\n",
    "print(\"ğŸš€ Ready for federated learning simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Simple Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for the federated learning simulation\n",
    "CONFIG = {\n",
    "    'num_clients': 6,\n",
    "    'num_rounds': 20,\n",
    "    'drift_round': 12,\n",
    "    'affected_clients': [2, 4],\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.01,\n",
    "    'local_epochs': 2,\n",
    "    'data_size': 1000,  # Samples per client\n",
    "    'input_dim': 20,\n",
    "    'num_classes': 4\n",
    "}\n",
    "\n",
    "print(\"ğŸ“Š Federated Learning Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "    \n",
    "print(f\"\\nâ±ï¸ Expected runtime: ~5-10 minutes\")\n",
    "print(f\"ğŸ’¾ Zero external downloads required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  Simple Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    \"\"\"Simple neural network for federated learning demo.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        \"\"\"Get model parameters as list of tensors.\"\"\"\n",
    "        return [param.data.clone() for param in self.parameters()]\n",
    "    \n",
    "    def set_parameters(self, parameters):\n",
    "        \"\"\"Set model parameters from list of tensors.\"\"\"\n",
    "        for param, new_param in zip(self.parameters(), parameters):\n",
    "            param.data.copy_(new_param)\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"Create a new model instance.\"\"\"\n",
    "    return SimpleClassifier(CONFIG['input_dim'], CONFIG['num_classes']).to(device)\n",
    "\n",
    "print(\"âœ… Neural network model ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(num_samples, input_dim, num_classes, drift=False, drift_intensity=0.3):\n",
    "    \"\"\"Generate synthetic classification data.\"\"\"\n",
    "    \n",
    "    # Generate features\n",
    "    X = torch.randn(num_samples, input_dim)\n",
    "    \n",
    "    # Generate labels with some pattern\n",
    "    weights = torch.randn(input_dim)\n",
    "    scores = X @ weights\n",
    "    \n",
    "    if drift:\n",
    "        # Add drift by changing the relationship\n",
    "        drift_weights = torch.randn(input_dim) * drift_intensity\n",
    "        scores = scores + X @ drift_weights\n",
    "        \n",
    "        # Add label noise\n",
    "        noise_mask = torch.rand(num_samples) < drift_intensity\n",
    "        labels = (scores > 0).long() + torch.randint(0, num_classes, (num_samples,))\n",
    "        labels = torch.where(noise_mask, torch.randint(0, num_classes, (num_samples,)), labels)\n",
    "    else:\n",
    "        # Normal relationship\n",
    "        labels = (scores > scores.median()).long()\n",
    "        \n",
    "        # Create multi-class labels\n",
    "        quartiles = torch.quantile(scores, torch.tensor([0.25, 0.5, 0.75]))\n",
    "        labels = torch.zeros(num_samples, dtype=torch.long)\n",
    "        labels[scores <= quartiles[0]] = 0\n",
    "        labels[(scores > quartiles[0]) & (scores <= quartiles[1])] = 1\n",
    "        labels[(scores > quartiles[1]) & (scores <= quartiles[2])] = 2\n",
    "        labels[scores > quartiles[2]] = 3\n",
    "    \n",
    "    labels = labels % num_classes  # Ensure valid class range\n",
    "    return X.float(), labels.long()\n",
    "\n",
    "def create_federated_datasets():\n",
    "    \"\"\"Create federated data splits for all clients.\"\"\"\n",
    "    client_datasets = {}\n",
    "    \n",
    "    for client_id in range(CONFIG['num_clients']):\n",
    "        # Generate data for each client\n",
    "        X, y = generate_synthetic_data(\n",
    "            CONFIG['data_size'], \n",
    "            CONFIG['input_dim'], \n",
    "            CONFIG['num_classes']\n",
    "        )\n",
    "        \n",
    "        dataset = TensorDataset(X, y)\n",
    "        client_datasets[client_id] = dataset\n",
    "        \n",
    "        print(f\"ğŸ‘¤ Client {client_id}: {len(dataset)} samples\")\n",
    "    \n",
    "    # Create test dataset\n",
    "    X_test, y_test = generate_synthetic_data(\n",
    "        500, CONFIG['input_dim'], CONFIG['num_classes']\n",
    "    )\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    print(f\"ğŸ§ª Test set: {len(test_dataset)} samples\")\n",
    "    return client_datasets, test_dataset\n",
    "\n",
    "print(\"âœ… Data generation functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Drift Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDriftDetector:\n",
    "    \"\"\"Simple drift detector using accuracy monitoring.\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=3, threshold=0.05):\n",
    "        self.window_size = window_size\n",
    "        self.threshold = threshold\n",
    "        self.accuracy_history = []\n",
    "        \n",
    "    def update(self, accuracy):\n",
    "        \"\"\"Update with new accuracy and check for drift.\"\"\"\n",
    "        self.accuracy_history.append(accuracy)\n",
    "        \n",
    "        if len(self.accuracy_history) < self.window_size * 2:\n",
    "            return False\n",
    "        \n",
    "        # Compare recent vs older accuracy\n",
    "        recent_avg = sum(self.accuracy_history[-self.window_size:]) / self.window_size\n",
    "        older_avg = sum(self.accuracy_history[-self.window_size*2:-self.window_size]) / self.window_size\n",
    "        \n",
    "        # Detect significant drop\n",
    "        drift_detected = (older_avg - recent_avg) > self.threshold\n",
    "        return drift_detected\n",
    "    \n",
    "    def get_recent_accuracy(self):\n",
    "        \"\"\"Get most recent accuracy.\"\"\"\n",
    "        return self.accuracy_history[-1] if self.accuracy_history else 0.0\n",
    "\n",
    "def inject_drift_to_client(client_id, client_datasets):\n",
    "    \"\"\"Inject drift to a specific client by regenerating their data.\"\"\"\n",
    "    print(f\"ğŸ’¥ Injecting drift to client {client_id}\")\n",
    "    \n",
    "    # Generate new drifted data\n",
    "    X_drift, y_drift = generate_synthetic_data(\n",
    "        CONFIG['data_size'], \n",
    "        CONFIG['input_dim'], \n",
    "        CONFIG['num_classes'],\n",
    "        drift=True,\n",
    "        drift_intensity=0.4\n",
    "    )\n",
    "    \n",
    "    # Replace client's dataset\n",
    "    client_datasets[client_id] = TensorDataset(X_drift, y_drift)\n",
    "    \n",
    "    return client_datasets\n",
    "\n",
    "print(\"âœ… Drift detection ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‘¥ Federated Learning Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedClient:\n",
    "    \"\"\"Simple federated learning client.\"\"\"\n",
    "    \n",
    "    def __init__(self, client_id, dataset, test_dataset):\n",
    "        self.client_id = client_id\n",
    "        self.dataset = dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.model = create_model()\n",
    "        self.drift_detector = SimpleDriftDetector()\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            dataset, batch_size=CONFIG['batch_size'], shuffle=True\n",
    "        )\n",
    "        self.test_loader = DataLoader(\n",
    "            test_dataset, batch_size=CONFIG['batch_size'], shuffle=False\n",
    "        )\n",
    "    \n",
    "    def train(self, global_parameters):\n",
    "        \"\"\"Train the local model.\"\"\"\n",
    "        # Set global parameters\n",
    "        self.model.set_parameters(global_parameters)\n",
    "        \n",
    "        # Train locally\n",
    "        self.model.train()\n",
    "        optimizer = optim.SGD(self.model.parameters(), lr=CONFIG['learning_rate'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for epoch in range(CONFIG['local_epochs']):\n",
    "            for batch_X, batch_y in self.train_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        train_accuracy = 100 * correct / total\n",
    "        \n",
    "        # Detect drift\n",
    "        drift_detected = self.drift_detector.update(train_accuracy / 100)\n",
    "        \n",
    "        return {\n",
    "            'parameters': self.model.get_parameters(),\n",
    "            'num_samples': len(self.dataset),\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'train_loss': total_loss / (len(self.train_loader) * CONFIG['local_epochs']),\n",
    "            'drift_detected': drift_detected\n",
    "        }\n",
    "    \n",
    "    def evaluate(self, global_parameters):\n",
    "        \"\"\"Evaluate the model.\"\"\"\n",
    "        self.model.set_parameters(global_parameters)\n",
    "        self.model.eval()\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in self.test_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        return {\n",
    "            'accuracy': 100 * correct / total,\n",
    "            'loss': total_loss / len(self.test_loader),\n",
    "            'num_samples': total\n",
    "        }\n",
    "\n",
    "print(\"âœ… Federated client ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ–¥ï¸ Federated Server with Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedServer:\n",
    "    \"\"\"Simple federated learning server.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.global_model = create_model()\n",
    "        self.drift_detected = False\n",
    "        self.round_metrics = []\n",
    "    \n",
    "    def aggregate_parameters(self, client_results, use_robust=False):\n",
    "        \"\"\"Aggregate client parameters using FedAvg or robust aggregation.\"\"\"\n",
    "        if not client_results:\n",
    "            return self.global_model.get_parameters()\n",
    "        \n",
    "        # Calculate total samples\n",
    "        total_samples = sum(result['num_samples'] for result in client_results)\n",
    "        \n",
    "        if use_robust:\n",
    "            # Robust aggregation: exclude worst performing clients\n",
    "            client_results_sorted = sorted(\n",
    "                client_results, \n",
    "                key=lambda x: x['train_accuracy'], \n",
    "                reverse=True\n",
    "            )\n",
    "            # Use top 70% of clients\n",
    "            keep_count = max(1, int(len(client_results_sorted) * 0.7))\n",
    "            client_results = client_results_sorted[:keep_count]\n",
    "            total_samples = sum(result['num_samples'] for result in client_results)\n",
    "            print(f\"ğŸ›¡ï¸ Robust aggregation: using {keep_count}/{len(client_results_sorted)} clients\")\n",
    "        \n",
    "        # Weighted average of parameters\n",
    "        aggregated_params = []\n",
    "        \n",
    "        for i in range(len(client_results[0]['parameters'])):\n",
    "            weighted_param = torch.zeros_like(client_results[0]['parameters'][i])\n",
    "            \n",
    "            for result in client_results:\n",
    "                weight = result['num_samples'] / total_samples\n",
    "                weighted_param += weight * result['parameters'][i]\n",
    "            \n",
    "            aggregated_params.append(weighted_param)\n",
    "        \n",
    "        return aggregated_params\n",
    "    \n",
    "    def check_global_drift(self, client_results):\n",
    "        \"\"\"Check if global drift should trigger robust aggregation.\"\"\"\n",
    "        if not client_results:\n",
    "            return False\n",
    "        \n",
    "        # Count clients detecting drift\n",
    "        drift_count = sum(1 for result in client_results if result['drift_detected'])\n",
    "        drift_rate = drift_count / len(client_results)\n",
    "        \n",
    "        # Trigger if >30% of clients detect drift\n",
    "        return drift_rate > 0.3\n",
    "    \n",
    "    def aggregate_round(self, client_results):\n",
    "        \"\"\"Perform one round of aggregation.\"\"\"\n",
    "        if not client_results:\n",
    "            return\n",
    "        \n",
    "        # Check for drift\n",
    "        global_drift = self.check_global_drift(client_results)\n",
    "        \n",
    "        if global_drift and not self.drift_detected:\n",
    "            print(\"ğŸš¨ GLOBAL DRIFT DETECTED! Switching to robust aggregation\")\n",
    "            self.drift_detected = True\n",
    "        \n",
    "        # Aggregate parameters\n",
    "        aggregated_params = self.aggregate_parameters(\n",
    "            client_results, use_robust=self.drift_detected\n",
    "        )\n",
    "        \n",
    "        # Update global model\n",
    "        self.global_model.set_parameters(aggregated_params)\n",
    "        \n",
    "        return aggregated_params\n",
    "    \n",
    "    def get_global_parameters(self):\n",
    "        \"\"\"Get current global model parameters.\"\"\"\n",
    "        return self.global_model.get_parameters()\n",
    "\n",
    "print(\"âœ… Federated server ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ® Main Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_federated_simulation():\n",
    "    \"\"\"Run the complete federated learning simulation.\"\"\"\n",
    "    print(\"ğŸš€ Starting Ultra-Simple Federated Learning Simulation!\")\n",
    "    print(f\"ğŸ“Š {CONFIG['num_clients']} clients, {CONFIG['num_rounds']} rounds\")\n",
    "    print(f\"ğŸ’¥ Drift injection at round {CONFIG['drift_round']}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create data and clients\n",
    "    client_datasets, test_dataset = create_federated_datasets()\n",
    "    \n",
    "    # Create server\n",
    "    server = FederatedServer()\n",
    "    \n",
    "    # Create clients\n",
    "    clients = {}\n",
    "    for client_id in range(CONFIG['num_clients']):\n",
    "        clients[client_id] = FederatedClient(\n",
    "            client_id, client_datasets[client_id], test_dataset\n",
    "        )\n",
    "    \n",
    "    # Track metrics\n",
    "    history = {\n",
    "        'rounds': [],\n",
    "        'global_accuracy': [],\n",
    "        'global_loss': [],\n",
    "        'fairness_gap': [],\n",
    "        'drift_detected': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for round_num in range(1, CONFIG['num_rounds'] + 1):\n",
    "        print(f\"\\nğŸ”„ Round {round_num}/{CONFIG['num_rounds']}\")\n",
    "        \n",
    "        # Inject drift if needed\n",
    "        if round_num == CONFIG['drift_round']:\n",
    "            for affected_client in CONFIG['affected_clients']:\n",
    "                client_datasets = inject_drift_to_client(affected_client, client_datasets)\n",
    "                # Update client with new data\n",
    "                clients[affected_client] = FederatedClient(\n",
    "                    affected_client, client_datasets[affected_client], test_dataset\n",
    "                )\n",
    "        \n",
    "        # Get global parameters\n",
    "        global_params = server.get_global_parameters()\n",
    "        \n",
    "        # Client training\n",
    "        client_results = []\n",
    "        for client_id, client in clients.items():\n",
    "            result = client.train(global_params)\n",
    "            client_results.append(result)\n",
    "        \n",
    "        # Server aggregation\n",
    "        server.aggregate_round(client_results)\n",
    "        \n",
    "        # Evaluation\n",
    "        global_params = server.get_global_parameters()\n",
    "        eval_results = []\n",
    "        \n",
    "        for client_id, client in clients.items():\n",
    "            eval_result = client.evaluate(global_params)\n",
    "            eval_results.append(eval_result)\n",
    "        \n",
    "        # Calculate global metrics\n",
    "        total_samples = sum(r['num_samples'] for r in eval_results)\n",
    "        global_accuracy = sum(r['accuracy'] * r['num_samples'] for r in eval_results) / total_samples\n",
    "        global_loss = sum(r['loss'] * r['num_samples'] for r in eval_results) / total_samples\n",
    "        \n",
    "        accuracies = [r['accuracy'] for r in eval_results]\n",
    "        fairness_gap = max(accuracies) - min(accuracies)\n",
    "        \n",
    "        # Store metrics\n",
    "        history['rounds'].append(round_num)\n",
    "        history['global_accuracy'].append(global_accuracy)\n",
    "        history['global_loss'].append(global_loss)\n",
    "        history['fairness_gap'].append(fairness_gap)\n",
    "        history['drift_detected'].append(server.drift_detected)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"ğŸ“Š Global Accuracy: {global_accuracy:.2f}%\")\n",
    "        print(f\"âš–ï¸ Fairness Gap: {fairness_gap:.2f}%\")\n",
    "        \n",
    "        # Show drift detection status\n",
    "        drift_count = sum(1 for r in client_results if r['drift_detected'])\n",
    "        if drift_count > 0:\n",
    "            print(f\"ğŸ” Drift signals: {drift_count}/{len(client_results)} clients\")\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    print(f\"\\nâœ… Simulation completed in {execution_time:.1f} seconds!\")\n",
    "    \n",
    "    return history, server\n",
    "\n",
    "print(\"âœ… Simulation function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(history):\n",
    "    \"\"\"Analyze and visualize simulation results.\"\"\"\n",
    "    print(\"ğŸ“Š Analyzing federated learning results...\")\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    rounds = history['rounds']\n",
    "    \n",
    "    # Plot 1: Global Accuracy\n",
    "    ax1.plot(rounds, history['global_accuracy'], 'b-', linewidth=2, marker='o')\n",
    "    ax1.axvline(x=CONFIG['drift_round'], color='red', linestyle='--', alpha=0.7, \n",
    "                label=f'Drift Injection (Round {CONFIG[\"drift_round\"]})')\n",
    "    ax1.set_xlabel('Round')\n",
    "    ax1.set_ylabel('Accuracy (%)')\n",
    "    ax1.set_title('ğŸ¯ Global Model Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Global Loss\n",
    "    ax2.plot(rounds, history['global_loss'], 'r-', linewidth=2, marker='s')\n",
    "    ax2.axvline(x=CONFIG['drift_round'], color='red', linestyle='--', alpha=0.7)\n",
    "    ax2.set_xlabel('Round')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('ğŸ“‰ Global Model Loss')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Fairness Gap\n",
    "    ax3.plot(rounds, history['fairness_gap'], 'g-', linewidth=2, marker='^')\n",
    "    ax3.axvline(x=CONFIG['drift_round'], color='red', linestyle='--', alpha=0.7)\n",
    "    ax3.set_xlabel('Round')\n",
    "    ax3.set_ylabel('Fairness Gap (%)')\n",
    "    ax3.set_title('âš–ï¸ Client Fairness Gap')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Drift Detection Status\n",
    "    drift_status = [1 if d else 0 for d in history['drift_detected']]\n",
    "    ax4.fill_between(rounds, drift_status, alpha=0.3, color='orange', label='Drift Mitigation Active')\n",
    "    ax4.axvline(x=CONFIG['drift_round'], color='red', linestyle='--', alpha=0.7)\n",
    "    ax4.set_xlabel('Round')\n",
    "    ax4.set_ylabel('Status')\n",
    "    ax4.set_title('ğŸ›¡ï¸ Drift Detection & Mitigation')\n",
    "    ax4.set_ylim(-0.1, 1.1)\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ¯ FEDERATED LEARNING SIMULATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if history['global_accuracy']:\n",
    "        final_accuracy = history['global_accuracy'][-1]\n",
    "        peak_accuracy = max(history['global_accuracy'])\n",
    "        final_fairness = history['fairness_gap'][-1]\n",
    "        \n",
    "        print(f\"ğŸ“Š Final Global Accuracy: {final_accuracy:.2f}%\")\n",
    "        print(f\"ğŸ“ˆ Peak Accuracy: {peak_accuracy:.2f}%\")\n",
    "        print(f\"âš–ï¸ Final Fairness Gap: {final_fairness:.2f}%\")\n",
    "        \n",
    "        # Calculate recovery metrics if drift was injected\n",
    "        if len(history['global_accuracy']) > CONFIG['drift_round']:\n",
    "            pre_drift_acc = history['global_accuracy'][CONFIG['drift_round']-2]  # Before drift\n",
    "            post_drift_acc = final_accuracy\n",
    "            recovery_rate = (post_drift_acc / pre_drift_acc) if pre_drift_acc > 0 else 0\n",
    "            \n",
    "            print(f\"ğŸ”„ Pre-drift accuracy: {pre_drift_acc:.2f}%\")\n",
    "            print(f\"ğŸ­ Post-drift accuracy: {post_drift_acc:.2f}%\")\n",
    "            print(f\"ğŸ’ª Recovery rate: {recovery_rate:.2%}\")\n",
    "        \n",
    "        drift_detected = any(history['drift_detected'])\n",
    "        print(f\"ğŸ›¡ï¸ Drift detection activated: {'Yes' if drift_detected else 'No'}\")\n",
    "        print(f\"ğŸ¯ Affected clients: {CONFIG['affected_clients']}\")\n",
    "        \n",
    "        print(\"\\nâœ… Key Observations:\")\n",
    "        print(\"   - Federated learning successfully aggregates knowledge across clients\")\n",
    "        print(\"   - Drift detection identifies performance degradation\")\n",
    "        print(\"   - Robust aggregation helps maintain model quality during drift\")\n",
    "        print(\"   - System demonstrates resilience to data distribution changes\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "print(\"âœ… Analysis function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Run the Complete Simulation\n",
    "\n",
    "**Execute this cell to run the federated learning experiment!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete federated learning simulation\n",
    "print(\"ğŸ¬ Starting Ultra-Simple Federated Learning Demo!\")\n",
    "print(\"ğŸ¯ Zero dependency conflicts - Pure PyTorch implementation\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Run simulation\n",
    "    history, server = run_federated_simulation()\n",
    "    \n",
    "    # Analyze results\n",
    "    analyze_results(history)\n",
    "    \n",
    "    print(\"\\nğŸ‰ Ultra-Simple Federated Learning Demo completed successfully!\")\n",
    "    print(\"\\nâœ… What you just saw:\")\n",
    "    print(\"   ğŸ”„ Multi-client federated training\")\n",
    "    print(\"   ğŸ’¥ Synthetic drift injection and detection\")\n",
    "    print(\"   ğŸ›¡ï¸ Robust aggregation for drift mitigation\")\n",
    "    print(\"   ğŸ“Š Real-time performance monitoring\")\n",
    "    print(\"   ğŸ“ˆ Recovery analysis and visualization\")\n",
    "    \n",
    "    print(\"\\nğŸ¯ This demonstrates the core concepts of federated learning\")\n",
    "    print(\"   with drift detection using only basic PyTorch!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Simulation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nğŸ’¡ If this fails, try restarting the runtime and running again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ Understanding the Results\n",
    "\n",
    "### ğŸ” **What Each Plot Shows:**\n",
    "\n",
    "1. **ğŸ¯ Global Model Accuracy**: Shows how well the federated model performs over time\n",
    "   - Should improve in early rounds\n",
    "   - May drop after drift injection (red line)\n",
    "   - Should recover with robust aggregation\n",
    "\n",
    "2. **ğŸ“‰ Global Model Loss**: Training loss across all clients\n",
    "   - Should decrease over time (better learning)\n",
    "   - May spike after drift injection\n",
    "\n",
    "3. **âš–ï¸ Client Fairness Gap**: Difference between best and worst client performance\n",
    "   - Lower is better (more fair)\n",
    "   - May increase after drift injection\n",
    "\n",
    "4. **ğŸ›¡ï¸ Drift Detection & Mitigation**: Shows when robust aggregation is active\n",
    "   - Orange area indicates drift mitigation is active\n",
    "   - Should activate after drift detection\n",
    "\n",
    "### ğŸ¯ **Key Concepts Demonstrated:**\n",
    "\n",
    "- **Federated Learning**: Multiple clients train locally, share model updates\n",
    "- **Drift Detection**: Monitors for data distribution changes\n",
    "- **Robust Aggregation**: Excludes poor-performing clients during drift\n",
    "- **Recovery**: System adapts and recovers from data drift\n",
    "\n",
    "### ğŸš€ **Experiment with Different Settings:**\n",
    "\n",
    "Try changing the `CONFIG` values above to see different behaviors:\n",
    "- `num_clients`: More clients = more federated complexity\n",
    "- `drift_round`: Earlier/later drift injection\n",
    "- `affected_clients`: More clients affected by drift\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Congratulations! You've successfully run a complete federated learning system with drift detection using only PyTorch!**"
   ]
  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.5\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n
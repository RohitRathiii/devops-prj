{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## üìñ Usage Instructions & Tips\n\n### üöÄ **How to Run**\n1. **Setup**: Run all cells in sequence starting from dependencies installation\n2. **Configuration**: Modify the `CONFIG` dictionary if needed (optional)\n3. **Quick Test**: Use the test configuration for faster runs during development\n4. **Main Execution**: Run the main execution cell to start the complete simulation\n5. **Results**: View automated visualizations and performance metrics\n\n### üéõÔ∏è **Configuration Options**\n\n**Model Settings:**\n- `model_name`: BERT model variant ('prajjwal1/bert-tiny' for speed)\n- `batch_size`: Training batch size (16 recommended for Colab)\n- `learning_rate`: Learning rate (2e-5 is optimal for BERT fine-tuning)\n\n**Federated Learning:**\n- `num_clients`: Number of federated clients (10 default)\n- `alpha`: Dirichlet concentration for non-IID data (0.5 = moderate heterogeneity)\n- `num_rounds`: Total training rounds (50 for complete experiment)\n\n**Drift Configuration:**\n- `injection_round`: When to inject drift (25 = halfway point)\n- `affected_clients`: Which clients receive drift ([2, 5, 8])\n- `drift_types`: Types of drift (['label_noise', 'vocab_shift'])\n- `drift_intensity`: Severity of drift (0.3 = 30% of data affected)\n\n**Detection Settings:**\n- `adwin_delta`: ADWIN sensitivity (0.002 = high sensitivity)\n- `mmd_p_val`: MMD significance threshold (0.05)\n- `trimmed_beta`: FedTrimmedAvg robustness (0.2 = trim 20% extremes)\n\n### üí° **Performance Tips**\n\n**For Faster Execution:**\n- Reduce `num_clients` to 5-8\n- Reduce `num_rounds` to 20-30\n- Use smaller `batch_size` (8-12)\n\n**For Better Results:**\n- Increase `num_rounds` to 60-100\n- Use more `affected_clients` for stronger drift signal\n- Experiment with different `drift_types`\n\n**GPU Optimization:**\n- The notebook automatically uses mixed precision (FP16) on GPU\n- Memory usage is optimized for T4/P100 GPUs\n- CPU fallback is available but slower\n\n### üìä **Expected Results**\n\n**Normal Scenario:**\n- Pre-drift accuracy: ~85-90%\n- Post-drift drop: ~5-15% \n- Recovery rate: ~80-95%\n- Detection delay: 1-3 rounds\n\n**Key Metrics:**\n- **Global Accuracy**: Weighted average across all clients\n- **Fairness Gap**: Difference between best and worst client performance\n- **Detection Rate**: Percentage of drift events successfully detected\n- **Recovery Rate**: How well the system recovers from drift\n\n### üîß **Troubleshooting**\n\n**Common Issues:**\n- **Memory Error**: Reduce batch_size or num_clients\n- **Slow Execution**: Enable GPU runtime in Colab settings\n- **Import Errors**: Restart runtime and reinstall dependencies\n- **NLTK Issues**: Vocabulary drift will fallback to simpler augmentation\n\n**Performance Monitoring:**\n- Watch GPU memory usage in Colab\n- Monitor training progress in real-time logs\n- Check drift detection alerts during execution\n\n---\n\n**üéØ Ready to explore federated learning drift detection? Run the main execution cell above!**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ‚ö° OPTIONAL: Quick Test Configuration\n# Uncomment and run this cell for a faster test run\n\n# TEST_CONFIG = CONFIG.copy()\n# TEST_CONFIG['simulation']['num_rounds'] = 20  # Reduce rounds for testing\n# TEST_CONFIG['federated']['num_clients'] = 5   # Reduce clients for faster execution\n# TEST_CONFIG['drift']['injection_round'] = 10  # Earlier drift injection\n# TEST_CONFIG['drift']['affected_clients'] = [1, 3]  # Fewer affected clients\n\n# print(\"üß™ Test configuration loaded:\")\n# print(f\"   üìä Clients: {TEST_CONFIG['federated']['num_clients']}\")\n# print(f\"   üîÑ Rounds: {TEST_CONFIG['simulation']['num_rounds']}\")\n# print(f\"   üí• Drift at round: {TEST_CONFIG['drift']['injection_round']}\")\n\n# # To use test config, replace CONFIG with TEST_CONFIG in the main execution cell",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ‚ö° Quick Test & Configuration Options\n\nOptional: Run a smaller test simulation or modify configuration parameters before the main execution.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# üöÄ MAIN EXECUTION - Run Federated Learning Drift Detection Simulation\n\nprint(\"üîÑ Initializing Federated Learning Drift Detection Simulation...\")\nprint(f\"üéÆ Device: {device}\")\nprint(f\"üíæ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\\\" if device.type == 'cuda' else \\\"CPU\\\")\")\n\n# Create and run simulation\nsimulation = FederatedDriftSimulation(CONFIG)\n\ntry:\n    # Start the simulation\n    print(\"\\\\nüöÄ Starting simulation - this may take 10-30 minutes depending on GPU...\")\n    start_time = time.time()\n    \n    # Run the complete simulation\n    results = simulation.run_simulation()\n    \n    # Calculate execution time\n    execution_time = time.time() - start_time\n    results['execution_time_minutes'] = execution_time / 60\n    \n    print(f\\\"\\\\n‚úÖ Simulation completed in {execution_time/60:.1f} minutes!\\\")\n    \n    # Print summary\n    print_simulation_summary(results)\n    \n    # Create visualizations\n    print(\\\"\\\\nüìà Creating visualizations...\\\")\n    create_comprehensive_visualizations(results)\n    \n    print(\\\"\\\\nüéâ Analysis complete! Results are displayed above.\\\")\n    \nexcept KeyboardInterrupt:\n    print(\\\"\\\\n‚è∏Ô∏è Simulation interrupted by user\\\")\nexcept Exception as e:\n    print(f\\\"\\\\n‚ùå Simulation failed with error: {e}\\\")\n    import traceback\n    traceback.print_exc()\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üöÄ Execute Federated Learning Simulation\n\nRun the complete drift detection and recovery experiment with real-time monitoring.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def create_comprehensive_visualizations(results: Dict[str, Any]):\n    \"\"\"Create comprehensive visualizations of simulation results.\"\"\"\n    \n    if 'round_metrics' not in results or not results['round_metrics']:\n        print(\"‚ö†Ô∏è No round metrics available for visualization\")\n        return\n    \n    # Create DataFrame from results\n    df = pd.DataFrame(results['round_metrics'])\n    config = results['config']\n    drift_round = config['drift']['injection_round']\n    \n    # Set up the plotting style\n    plt.style.use('default')\n    sns.set_palette(\"husl\")\n    \n    # Create comprehensive plot\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n    fig.suptitle(f'üîÑ Federated Learning Drift Detection Results\\\\nSimulation ID: {results[\"simulation_id\"]}', \n                 fontsize=16, fontweight='bold')\n    \n    # 1. Global Accuracy Over Time\n    ax1 = axes[0, 0]\n    ax1.plot(df['round'], df['global_accuracy'], 'b-', linewidth=2, label='Global Accuracy')\n    ax1.axvline(x=drift_round, color='red', linestyle='--', alpha=0.7, label=f'Drift Injection (R{drift_round})')\n    \n    # Highlight mitigation period if available\n    if 'drift_summary' in results and 'drift_rounds' in results['drift_summary']:\n        drift_rounds = results['drift_summary']['drift_rounds']\n        for dr in drift_rounds:\n            ax1.axvline(x=dr, color='orange', linestyle=':', alpha=0.5, label='Drift Detected' if dr == drift_rounds[0] else \"\")\n    \n    ax1.set_xlabel('Round')\n    ax1.set_ylabel('Global Accuracy (%)')\n    ax1.set_title('üéØ Global Accuracy Trend')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # 2. Fairness Gap Analysis\n    ax2 = axes[0, 1]\n    ax2.plot(df['round'], df['fairness_gap'], 'g-', linewidth=2, label='Fairness Gap')\n    ax2.axvline(x=drift_round, color='red', linestyle='--', alpha=0.7)\n    ax2.fill_between(df['round'], 0, df['fairness_gap'], alpha=0.3, color='green')\n    \n    ax2.set_xlabel('Round')\n    ax2.set_ylabel('Fairness Gap (%)')\n    ax2.set_title('‚öñÔ∏è Client Fairness Gap')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    # 3. Accuracy Distribution Box Plot\n    ax3 = axes[1, 0]\n    \n    # Create accuracy distribution data\n    pre_drift_acc = df[df['round'] < drift_round]['global_accuracy'].tolist()\n    post_drift_acc = df[df['round'] >= drift_round]['global_accuracy'].tolist()\n    \n    box_data = []\n    labels = []\n    if pre_drift_acc:\n        box_data.append(pre_drift_acc)\n        labels.append(f'Pre-Drift\\\\n(R1-{drift_round-1})')\n    if post_drift_acc:\n        box_data.append(post_drift_acc)\n        labels.append(f'Post-Drift\\\\n(R{drift_round}+)')\n    \n    if box_data:\n        bp = ax3.boxplot(box_data, labels=labels, patch_artist=True)\n        colors = ['lightblue', 'lightcoral']\n        for patch, color in zip(bp['boxes'], colors[:len(box_data)]):\n            patch.set_facecolor(color)\n    \n    ax3.set_ylabel('Global Accuracy (%)')\n    ax3.set_title('üìä Accuracy Distribution')\n    ax3.grid(True, alpha=0.3)\n    \n    # 4. Performance Metrics Summary\n    ax4 = axes[1, 1]\n    ax4.axis('off')\n    \n    # Create performance summary\n    if 'performance_metrics' in results:\n        metrics = results['performance_metrics']\n        summary_text = f\\\"\\\"\\\"üìä PERFORMANCE SUMMARY\n        \nüéØ Final Accuracy: {metrics.get('final_accuracy', 0):.2f}%\nüìà Peak Accuracy: {metrics.get('peak_accuracy', 0):.2f}%\nüìâ Average Accuracy: {metrics.get('avg_accuracy', 0):.2f}%\n\n‚öñÔ∏è Final Fairness Gap: {metrics.get('final_fairness_gap', 0):.2f}%\nüî∫ Max Fairness Gap: {metrics.get('max_fairness_gap', 0):.2f}%\n\nüîÑ Pre-Drift Accuracy: {metrics.get('pre_drift_accuracy', 0):.2f}%\nüé≠ Post-Drift Accuracy: {metrics.get('post_drift_accuracy', 0):.2f}%\nüí™ Recovery Rate: {metrics.get('accuracy_recovery_rate', 0):.2f}\\\"\\\"\\\"\\n    \\n        ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes, \\n                fontsize=12, verticalalignment='top',\\n                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))\\n    \\n    # Drift detection summary\\n    if 'drift_summary' in results:\\n        drift_info = results['drift_summary']\\n        drift_text = f\\\"\\\"\\\"üîç DRIFT DETECTION SUMMARY\\n        \\nüìä Detection Rate: {drift_info.get('drift_detection_rate', 0):.2%}\\nüõ°Ô∏è Mitigation Active: {drift_info.get('mitigation_activated', False)}\\nüí• Drift Rounds: {drift_info.get('drift_rounds', [])}\\nüéØ Affected Clients: {config['drift']['affected_clients']}\\nüîÑ Drift Types: {config['drift']['drift_types']}\\\"\\\"\\\"\\n        \\n        ax4.text(0.1, 0.4, drift_text, transform=ax4.transAxes, \\n                fontsize=12, verticalalignment='top',\\n                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow', alpha=0.8))\\n    \\n    plt.tight_layout()\\n    plt.show()\\n    \\n    # Create additional drift timeline plot\\n    create_drift_timeline_plot(results)\\n\\n\\ndef create_drift_timeline_plot(results: Dict[str, Any]):\\n    \\\"\\\"\\\"Create detailed drift detection timeline.\\\"\\\"\\\"\\n    if 'round_metrics' not in results:\\n        return\\n        \\n    df = pd.DataFrame(results['round_metrics'])\\n    config = results['config']\\n    \\n    plt.figure(figsize=(14, 8))\\n    \\n    # Main accuracy plot\\n    plt.subplot(2, 1, 1)\\n    plt.plot(df['round'], df['global_accuracy'], 'b-', linewidth=2, label='Global Accuracy')\\n    plt.axvline(x=config['drift']['injection_round'], color='red', linestyle='--', alpha=0.7, \\n                label=f'Drift Injection (R{config[\\\"drift\\\"][\\\"injection_round\\\"]})')\\n    \\n    # Mark drift detection points\\n    if 'drift_summary' in results and 'drift_rounds' in results['drift_summary']:\\n        for dr in results['drift_summary']['drift_rounds']:\\n            plt.axvline(x=dr, color='orange', linestyle=':', alpha=0.8, linewidth=2)\\n    \\n    plt.xlabel('Round')\\n    plt.ylabel('Global Accuracy (%)')\\n    plt.title('üîç Drift Detection Timeline')\\n    plt.legend()\\n    plt.grid(True, alpha=0.3)\\n    \\n    # Fairness gap subplot\\n    plt.subplot(2, 1, 2)\\n    plt.plot(df['round'], df['fairness_gap'], 'g-', linewidth=2, label='Fairness Gap')\\n    plt.axvline(x=config['drift']['injection_round'], color='red', linestyle='--', alpha=0.7)\\n    \\n    if 'drift_summary' in results and 'drift_rounds' in results['drift_summary']:\\n        for dr in results['drift_summary']['drift_rounds']:\\n            plt.axvline(x=dr, color='orange', linestyle=':', alpha=0.8, linewidth=2)\\n    \\n    plt.xlabel('Round')\\n    plt.ylabel('Fairness Gap (%)')\\n    plt.title('‚öñÔ∏è Client Fairness Evolution')\\n    plt.legend()\\n    plt.grid(True, alpha=0.3)\\n    \\n    plt.tight_layout()\\n    plt.show()\\n\\n\\ndef print_simulation_summary(results: Dict[str, Any]):\\n    \\\"\\\"\\\"Print comprehensive simulation summary.\\\"\\\"\\\"\\n    print(\\\"\\\\n\\\" + \\\"=\\\"*80)\\n    print(\\\"üéØ FEDERATED LEARNING DRIFT DETECTION - SIMULATION SUMMARY\\\")\\n    print(\\\"=\\\"*80)\\n    \\n    print(f\\\"üÜî Simulation ID: {results['simulation_id']}\\\")\\n    print(f\\\"‚è∞ Completed: {results.get('completed_at', 'Unknown')}\\\")\\n    \\n    config = results['config']\\n    print(f\\\"\\\\nüìä CONFIGURATION:\\\")\\n    print(f\\\"   üë• Clients: {config['federated']['num_clients']}\\\")\\n    print(f\\\"   üîÑ Rounds: {config['simulation']['num_rounds']}\\\")\\n    print(f\\\"   üí• Drift Injection: Round {config['drift']['injection_round']}\\\")\\n    print(f\\\"   üéØ Affected Clients: {config['drift']['affected_clients']}\\\")\\n    print(f\\\"   üîÑ Drift Types: {config['drift']['drift_types']}\\\")\\n    \\n    if 'performance_metrics' in results:\\n        metrics = results['performance_metrics']\\n        print(f\\\"\\\\nüéØ PERFORMANCE METRICS:\\\")\\n        print(f\\\"   üìà Final Global Accuracy: {metrics.get('final_accuracy', 0):.2f}%\\\")\\n        print(f\\\"   üèÜ Peak Accuracy: {metrics.get('peak_accuracy', 0):.2f}%\\\")\\n        print(f\\\"   ‚öñÔ∏è Final Fairness Gap: {metrics.get('final_fairness_gap', 0):.2f}%\\\")\\n        \\n        if 'accuracy_recovery_rate' in metrics:\\n            print(f\\\"   üîÑ Recovery Rate: {metrics['accuracy_recovery_rate']:.2%}\\\")\\n            print(f\\\"   üìä Pre-Drift Accuracy: {metrics.get('pre_drift_accuracy', 0):.2f}%\\\")\\n            print(f\\\"   üé≠ Post-Drift Accuracy: {metrics.get('post_drift_accuracy', 0):.2f}%\\\")\\n    \\n    if 'drift_summary' in results:\\n        drift_summary = results['drift_summary']\\n        print(f\\\"\\\\nüîç DRIFT DETECTION SUMMARY:\\\")\\n        print(f\\\"   üìä Detection Rate: {drift_summary.get('drift_detection_rate', 0):.2%}\\\")\\n        print(f\\\"   üõ°Ô∏è Mitigation Activated: {drift_summary.get('mitigation_activated', False)}\\\")\\n        print(f\\\"   üí• Drift Detected at Rounds: {drift_summary.get('drift_rounds', [])}\\\")\\n    \\n    print(\\\"=\\\"*80)\\n\\n\\nprint(\\\"‚úÖ Visualization components ready!\\\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üìà Visualization and Results Analysis\n\nInteractive visualizations for monitoring drift detection and federated learning performance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class FederatedDriftSimulation:\n    \"\"\"Main simulation orchestrator for federated learning with drift detection.\"\"\"\n\n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self.device = device  # Use global device\n        \n        # Initialize components\n        self.data_loader = None\n        self.client_datasets = {}\n        self.test_dataset = None\n        self.drift_injector = DriftInjector(config['drift']['drift_intensity'])\n        \n        # Simulation state\n        self.simulation_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        self.results = {'simulation_id': self.simulation_id, 'config': config}\n        \n        print(f\"üöÄ Simulation {self.simulation_id} initialized\")\n\n    def prepare_data(self):\n        \"\"\"Prepare federated datasets.\"\"\"\n        print(\"üìä Preparing federated datasets...\")\n        \n        # Create data loader\n        self.data_loader = FederatedDataLoader(\n            num_clients=self.config['federated']['num_clients'],\n            alpha=self.config['federated']['alpha'],\n            batch_size=self.config['model']['batch_size']\n        )\n        \n        # Create federated splits\n        self.client_datasets, self.test_dataset = self.data_loader.create_federated_splits(self.config)\n        \n        print(f\"‚úÖ Data prepared: {len(self.client_datasets)} clients, test set size: {len(self.test_dataset)}\")\n\n    def create_client_fn(self):\n        \"\"\"Create client factory function for Flower simulation.\"\"\"\n        config = self.config\n        device = self.device\n        client_datasets = self.client_datasets\n        test_dataset = self.test_dataset\n        drift_injector = self.drift_injector\n        drift_injection_round = self.config['drift']['injection_round']\n        affected_clients = set(self.config['drift']['affected_clients'])\n        drift_types = self.config['drift']['drift_types']\n        \n        # Track drift injection state\n        drift_state = {'injected': False}\n\n        def client_fn(context: Context):\n            \"\"\"Create a client for the given context.\"\"\"\n            # Map Ray node ID to client index\n            client_idx = int(context.node_id) % len(client_datasets)\n            \n            # Get current round from context\n            current_round = getattr(context, 'round', 1)\n            \n            # Apply drift injection if needed\n            if (current_round >= drift_injection_round and \n                not drift_state['injected'] and \n                client_idx in affected_clients):\n                \n                print(f\"üí• Injecting drift to client {client_idx} at round {current_round}\")\n                original_dataset = client_datasets[client_idx]\n                client_datasets[client_idx] = drift_injector.apply_drift(original_dataset, drift_types)\n                drift_state['injected'] = True\n\n            # Create model for client\n            model, tokenizer = create_model_and_tokenizer(config, device)\n\n            # Get client's dataset\n            train_dataset = client_datasets[client_idx]\n            \n            # Create data loaders\n            train_loader = DataLoader(\n                train_dataset,\n                batch_size=config['model']['batch_size'],\n                shuffle=True,\n                drop_last=True\n            )\n            \n            test_loader = DataLoader(\n                test_dataset,\n                batch_size=config['model']['batch_size'],\n                shuffle=False\n            )\n\n            # Create drift-aware client\n            client = DriftAwareClient(\n                client_id=str(client_idx),\n                model=model,\n                train_loader=train_loader,\n                test_loader=test_loader,\n                device=device,\n                config=config\n            )\n\n            return client.to_client()\n\n        return client_fn\n\n    def run_simulation(self):\n        \"\"\"Run the complete federated learning simulation.\"\"\"\n        print(f\"üöÄ Starting federated learning simulation...\")\n        print(f\"üìä Configuration: {self.config['federated']['num_clients']} clients, {self.config['simulation']['num_rounds']} rounds\")\n        print(f\"üí• Drift injection: Round {self.config['drift']['injection_round']} ‚Üí Clients {self.config['drift']['affected_clients']}\")\n        \n        # Prepare data\n        self.prepare_data()\n        \n        # Create strategy\n        strategy = DriftAwareStrategy(\n            config=self.config,\n            fraction_fit=self.config['simulation']['fraction_fit'],\n            fraction_evaluate=self.config['simulation']['fraction_evaluate'],\n            min_fit_clients=self.config['simulation']['min_fit_clients'],\n            min_evaluate_clients=self.config['simulation']['min_evaluate_clients']\n        )\n        \n        # Create client function\n        client_fn = self.create_client_fn()\n        \n        # Run simulation\n        try:\n            print(\"üîÑ Starting Flower simulation...\")\n            \n            history = start_simulation(\n                client_fn=client_fn,\n                num_clients=self.config['federated']['num_clients'],\n                config=fl.server.ServerConfig(num_rounds=self.config['simulation']['num_rounds']),\n                strategy=strategy,\n                client_resources={\"num_cpus\": 1, \"num_gpus\": 0.1 if device.type == 'cuda' else 0.0},\n                ray_init_args={\"include_dashboard\": False, \"log_to_driver\": False}\n            )\n            \n            print(\"‚úÖ Simulation completed successfully!\")\n            \n            # Analyze results\n            self._analyze_results(history, strategy)\n            \n            return self.results\n            \n        except Exception as e:\n            print(f\"‚ùå Simulation failed: {e}\")\n            raise e\n\n    def _analyze_results(self, history, strategy):\n        \"\"\"Analyze simulation results and generate metrics.\"\"\"\n        print(\"üìä Analyzing simulation results...\")\n        \n        # Extract training history\n        if hasattr(history, 'metrics_centralized'):\n            rounds_data = []\n            for round_idx, (round_num, metrics) in enumerate(history.metrics_centralized):\n                rounds_data.append({\n                    'round': round_num,\n                    **metrics\n                })\n            self.results['round_metrics'] = rounds_data\n        \n        # Get drift detection summary\n        self.results['drift_summary'] = strategy.get_drift_summary()\n        \n        # Calculate performance metrics\n        if 'round_metrics' in self.results and self.results['round_metrics']:\n            metrics_df = pd.DataFrame(self.results['round_metrics'])\n            \n            performance_metrics = {\n                'final_accuracy': float(metrics_df['global_accuracy'].iloc[-1]),\n                'peak_accuracy': float(metrics_df['global_accuracy'].max()),\n                'avg_accuracy': float(metrics_df['global_accuracy'].mean()),\n                'final_fairness_gap': float(metrics_df['fairness_gap'].iloc[-1]),\n                'max_fairness_gap': float(metrics_df['fairness_gap'].max())\n            }\n            \n            # Calculate recovery metrics if drift was detected\n            drift_round = self.config['drift']['injection_round']\n            if len(metrics_df) > drift_round:\n                pre_drift_acc = metrics_df[metrics_df['round'] < drift_round]['global_accuracy'].mean()\n                post_drift_acc = metrics_df['global_accuracy'].iloc[-1]\n                performance_metrics['pre_drift_accuracy'] = float(pre_drift_acc)\n                performance_metrics['post_drift_accuracy'] = float(post_drift_acc)\n                performance_metrics['accuracy_recovery_rate'] = float(post_drift_acc / pre_drift_acc) if pre_drift_acc > 0 else 0.0\n            \n            self.results['performance_metrics'] = performance_metrics\n        \n        # Store final timestamp\n        self.results['completed_at'] = datetime.now().isoformat()\n        \n        print(\"‚úÖ Results analysis completed\")\n\n\nprint(\"‚úÖ Main simulation orchestrator ready!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üéÆ Main Simulation Orchestrator\n\nComplete federated learning simulation with drift injection and real-time monitoring.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class DriftAwareStrategy(FedAvg):\n    \"\"\"Drift-aware federated averaging with FedTrimmedAvg mitigation.\"\"\"\n\n    def __init__(self, config: Dict[str, Any], **kwargs):\n        super().__init__(**kwargs)\n        self.config = config\n        self.drift_config = config['drift_detection']\n        self.simulation_config = config['simulation']\n        \n        # Drift detection and mitigation state\n        self.drift_detector = DriftDetectionSystem(config)\n        self.mitigation_active = False\n        self.drift_history = []\n        self.global_embeddings_history = []\n        \n        # Performance tracking\n        self.round_metrics = []\n\n    def aggregate_fit(self, server_round: int, results, failures):\n        \"\"\"Aggregate client updates with drift detection and mitigation.\"\"\"\n        print(f\"\\nüîÑ Round {server_round}: Processing {len(results)} client updates\")\n        \n        # Extract drift signals from client results\n        drift_signals = self._extract_drift_signals(results)\n        \n        # Analyze global drift patterns\n        global_drift_detected = self._analyze_global_drift(server_round, drift_signals, results)\n        \n        # Store drift information\n        self.drift_history.append({\n            'round': server_round,\n            'client_drift_signals': drift_signals,\n            'global_drift': global_drift_detected,\n            'mitigation_active': self.mitigation_active\n        })\n\n        # Choose aggregation strategy based on drift detection\n        if global_drift_detected and not self.mitigation_active:\n            print(\"üõ°Ô∏è DRIFT DETECTED: Activating FedTrimmedAvg mitigation\")\n            self.mitigation_active = True\n            aggregated_weights = self._fed_trimmed_avg(results)\n        elif self.mitigation_active:\n            print(\"üõ°Ô∏è Continuing FedTrimmedAvg mitigation\")\n            aggregated_weights = self._fed_trimmed_avg(results)\n        else:\n            print(\"üìä Normal operation: Using FedAvg\")\n            # Use standard FedAvg\n            aggregated_weights = super().aggregate_fit(server_round, results, failures)[0]\n\n        return aggregated_weights, {}\n\n    def aggregate_evaluate(self, server_round: int, results, failures):\n        \"\"\"Aggregate evaluation results and compute metrics.\"\"\"\n        if not results:\n            return None, {}\n\n        # Calculate weighted average metrics\n        total_examples = sum(r[1] for r in results)\n        weighted_acc = sum(r[1] * r[2]['accuracy'] for r in results) / total_examples\n        weighted_loss = sum(r[0] * r[1] for r in results) / total_examples\n        \n        # Calculate fairness metrics\n        accuracies = [r[2]['accuracy'] for r in results]\n        fairness_gap = max(accuracies) - min(accuracies)\n        \n        metrics = {\n            'global_accuracy': weighted_acc,\n            'global_loss': weighted_loss,\n            'fairness_gap': fairness_gap,\n            'min_accuracy': min(accuracies),\n            'max_accuracy': max(accuracies),\n            'std_accuracy': np.std(accuracies)\n        }\n        \n        self.round_metrics.append({\n            'round': server_round,\n            **metrics\n        })\n        \n        print(f\"üìä Round {server_round} Metrics:\")\n        print(f\"   Global Accuracy: {weighted_acc:.2f}%\")\n        print(f\"   Fairness Gap: {fairness_gap:.2f}%\")\n        print(f\"   Mitigation Active: {self.mitigation_active}\")\n        \n        return weighted_loss, metrics\n\n    def _extract_drift_signals(self, results):\n        \"\"\"Extract drift detection signals from client results.\"\"\"\n        drift_signals = {}\n        \n        for client_proxy, fit_res in results:\n            if 'drift_signals' in fit_res.metrics:\n                drift_info = fit_res.metrics['drift_signals']\n                drift_signals[drift_info['client_id']] = drift_info\n        \n        return drift_signals\n\n    def _analyze_global_drift(self, server_round: int, drift_signals: Dict, results) -> bool:\n        \"\"\"Analyze global drift patterns across all clients.\"\"\"\n        if not drift_signals:\n            return False\n\n        # Count clients reporting concept drift\n        concept_drift_count = sum(1 for signals in drift_signals.values() \n                                if signals.get('concept_drift', False))\n        \n        concept_drift_rate = concept_drift_count / len(drift_signals) if drift_signals else 0\n\n        # Collect embeddings for MMD test\n        all_embeddings = []\n        for signals in drift_signals.values():\n            if 'embedding_sample' in signals and signals['embedding_sample']:\n                embeddings = np.array(signals['embedding_sample'])\n                if embeddings.shape[0] > 0:\n                    all_embeddings.append(embeddings)\n\n        mmd_drift_detected = False\n        if all_embeddings and len(self.global_embeddings_history) > 5:\n            try:\n                current_embeddings = np.vstack(all_embeddings)\n                # Use embeddings from 5 rounds ago as reference\n                reference_embeddings = self.global_embeddings_history[-5]\n                \n                if not hasattr(self, 'mmd_detector') or self.mmd_detector is None:\n                    self.drift_detector.setup_mmd_detector(reference_embeddings)\n                \n                mmd_result = self.drift_detector.detect_mmd_drift(current_embeddings)\n                mmd_drift_detected = mmd_result['is_drift']\n                \n                print(f\"üî¨ MMD Test: p-value={mmd_result['p_value']:.4f}, drift={mmd_drift_detected}\")\n                \n            except Exception as e:\n                print(f\"‚ö†Ô∏è MMD analysis failed: {e}\")\n\n        # Store current embeddings for future reference\n        if all_embeddings:\n            current_embeddings = np.vstack(all_embeddings)\n            self.global_embeddings_history.append(current_embeddings)\n            # Keep only recent history to manage memory\n            if len(self.global_embeddings_history) > 10:\n                self.global_embeddings_history.pop(0)\n\n        # Global drift decision\n        threshold = self.simulation_config['mitigation_threshold']\n        global_drift = (concept_drift_rate > threshold) or mmd_drift_detected\n        \n        print(f\"üîç Drift Analysis: concept_rate={concept_drift_rate:.2f}, mmd_drift={mmd_drift_detected}, global_drift={global_drift}\")\n        \n        return global_drift\n\n    def _fed_trimmed_avg(self, results):\n        \"\"\"Implement FedTrimmedAvg for robust aggregation.\"\"\"\n        beta = self.drift_config['trimmed_beta']\n        \n        # Extract weights and client sizes\n        weights_list = []\n        sizes = []\n        \n        for client_proxy, fit_res in results:\n            weights = fl.common.parameters_to_ndarrays(fit_res.parameters)\n            weights_list.append(weights)\n            sizes.append(fit_res.num_examples)\n        \n        if not weights_list:\n            return None\n        \n        # Convert to numpy arrays for easier manipulation\n        num_layers = len(weights_list[0])\n        aggregated_weights = []\n        \n        for layer_idx in range(num_layers):\n            # Stack all client weights for this layer\n            layer_weights = np.array([w[layer_idx] for w in weights_list])\n            layer_sizes = np.array(sizes)\n            \n            # Calculate weighted parameters\n            weighted_params = layer_weights * layer_sizes.reshape(-1, *([1] * (layer_weights.ndim - 1)))\n            \n            # Sort by parameter magnitude for trimming\n            param_norms = np.linalg.norm(weighted_params.reshape(len(weights_list), -1), axis=1)\n            sorted_indices = np.argsort(param_norms)\n            \n            # Trim extreme beta fraction from both ends\n            num_clients = len(weights_list)\n            num_to_trim = max(1, int(beta * num_clients))\n            \n            if num_clients > 2 * num_to_trim:\n                # Trim from both ends\n                start_idx = num_to_trim\n                end_idx = num_clients - num_to_trim\n                trimmed_indices = sorted_indices[start_idx:end_idx]\n            else:\n                # If too few clients, use all\n                trimmed_indices = sorted_indices\n            \n            # Aggregate trimmed weights\n            trimmed_weighted = weighted_params[trimmed_indices]\n            trimmed_sizes = layer_sizes[trimmed_indices]\n            \n            aggregated_layer = np.sum(trimmed_weighted, axis=0) / np.sum(trimmed_sizes)\n            aggregated_weights.append(aggregated_layer)\n        \n        print(f\"üõ°Ô∏è FedTrimmedAvg: trimmed {num_to_trim * 2}/{num_clients} clients\")\n        \n        return fl.common.ndarrays_to_parameters(aggregated_weights)\n\n    def get_drift_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary of drift detection results.\"\"\"\n        if not self.drift_history:\n            return {}\n        \n        total_rounds = len(self.drift_history)\n        drift_rounds = sum(1 for entry in self.drift_history if entry['global_drift'])\n        \n        return {\n            'total_rounds': total_rounds,\n            'drift_detection_rate': drift_rounds / total_rounds,\n            'mitigation_activated': self.mitigation_active,\n            'drift_rounds': [entry['round'] for entry in self.drift_history if entry['global_drift']]\n        }\n\n\nprint(\"‚úÖ Drift-aware server strategy ready!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class DriftAwareClient(fl.client.NumPyClient):\n    \"\"\"Federated learning client with integrated drift detection.\"\"\"\n\n    def __init__(self, client_id: str, model: BERTClassifier, train_loader: DataLoader,\n                 test_loader: DataLoader, device: torch.device, config: Dict[str, Any]):\n        self.client_id = client_id\n        self.model = model\n        self.train_loader = train_loader\n        self.test_loader = test_loader\n        self.device = device\n        self.config = config\n\n        # Initialize drift detection\n        self.drift_detector = DriftDetectionSystem(config)\n        \n        # Training setup\n        self.optimizer = optim.AdamW(model.parameters(), \n                                   lr=config['model']['learning_rate'])\n        self.scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n\n    def get_parameters(self, config):\n        \"\"\"Return model parameters.\"\"\"\n        return [param.cpu().numpy() for param in self.model.parameters()]\n\n    def set_parameters(self, parameters):\n        \"\"\"Set model parameters.\"\"\"\n        params_dict = zip(self.model.state_dict().keys(), parameters)\n        state_dict = {k: torch.tensor(v) for k, v in params_dict}\n        self.model.load_state_dict(state_dict, strict=True)\n\n    def fit(self, parameters, config):\n        \"\"\"Train model and detect drift.\"\"\"\n        # Set global parameters\n        self.set_parameters(parameters)\n        \n        # Train model\n        train_loss, train_acc = self._train()\n        \n        # Collect embeddings for drift detection\n        embeddings = self._collect_embeddings()\n        \n        # Check for concept drift using ADWIN\n        concept_drift = self.drift_detector.update_adwin(train_acc)\n        \n        # Prepare drift signals\n        drift_signals = {\n            'client_id': self.client_id,\n            'concept_drift': concept_drift,\n            'train_accuracy': train_acc,\n            'embedding_sample': embeddings[:100].tolist() if len(embeddings) > 0 else []\n        }\n\n        return (self.get_parameters({}), len(self.train_loader.dataset), \n                {'train_loss': train_loss, 'train_acc': train_acc, 'drift_signals': drift_signals})\n\n    def evaluate(self, parameters, config):\n        \"\"\"Evaluate model.\"\"\"\n        self.set_parameters(parameters)\n        \n        loss, accuracy = self._evaluate()\n        \n        return loss, len(self.test_loader.dataset), {'accuracy': accuracy}\n\n    def _train(self):\n        \"\"\"Training loop with mixed precision support.\"\"\"\n        self.model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n\n        for batch in self.train_loader:\n            # Move batch to device\n            input_ids = batch['input_ids'].to(self.device)\n            attention_mask = batch['attention_mask'].to(self.device)\n            labels = batch['labels'].to(self.device)\n\n            # Convert to appropriate dtype for mixed precision\n            if self.device.type == 'cuda':\n                input_ids = input_ids.long()  # Keep input_ids as long\n                attention_mask = attention_mask.long()  # Keep attention_mask as long\n\n            self.optimizer.zero_grad()\n\n            # Forward pass with mixed precision\n            if self.scaler is not None:\n                with torch.cuda.amp.autocast():\n                    outputs = self.model(input_ids, attention_mask, labels)\n                    loss = outputs['loss']\n                \n                # Backward pass with gradient scaling\n                self.scaler.scale(loss).backward()\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n            else:\n                outputs = self.model(input_ids, attention_mask, labels)\n                loss = outputs['loss']\n                loss.backward()\n                self.optimizer.step()\n\n            # Statistics\n            total_loss += loss.item()\n            _, predicted = torch.max(outputs['logits'], 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        avg_loss = total_loss / len(self.train_loader)\n        accuracy = 100 * correct / total\n        \n        return avg_loss, accuracy\n\n    def _evaluate(self):\n        \"\"\"Evaluation loop.\"\"\"\n        self.model.eval()\n        total_loss = 0\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for batch in self.test_loader:\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n\n                if self.device.type == 'cuda':\n                    input_ids = input_ids.long()\n                    attention_mask = attention_mask.long()\n\n                outputs = self.model(input_ids, attention_mask, labels)\n                loss = outputs['loss']\n\n                total_loss += loss.item()\n                _, predicted = torch.max(outputs['logits'], 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n        avg_loss = total_loss / len(self.test_loader)\n        accuracy = 100 * correct / total\n        \n        return avg_loss, accuracy\n\n    def _collect_embeddings(self, max_samples: int = 500):\n        \"\"\"Collect embeddings for drift detection.\"\"\"\n        self.model.eval()\n        embeddings = []\n        \n        with torch.no_grad():\n            for i, batch in enumerate(self.train_loader):\n                if i * self.config['model']['batch_size'] >= max_samples:\n                    break\n                    \n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n\n                if self.device.type == 'cuda':\n                    input_ids = input_ids.long()\n                    attention_mask = attention_mask.long()\n\n                batch_embeddings = self.model.get_embeddings(input_ids, attention_mask)\n                embeddings.append(batch_embeddings.cpu().numpy())\n\n        if embeddings:\n            return np.vstack(embeddings)\n        return np.array([])\n\n\nprint(\"‚úÖ Drift-aware client implementation ready!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üë• Federated Learning Components\n\nDrift-aware FL client and robust server strategy with FedTrimmedAvg mitigation.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class DriftDetectionSystem:\n    \"\"\"Multi-level drift detection system combining ADWIN, MMD, and Evidently.\"\"\"\n\n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self.drift_config = config['drift_detection']\n        \n        # Initialize ADWIN for concept drift\n        self.adwin = ADWIN(delta=self.drift_config['adwin_delta'])\n        \n        # Initialize drift state tracking\n        self.drift_history = []\n        self.reference_embeddings = None\n        self.mmd_detector = None\n\n    def update_adwin(self, performance_metric: float) -> bool:\n        \"\"\"Update ADWIN with performance metric and check for drift.\"\"\"\n        self.adwin.update(performance_metric)\n        return self.adwin.drift_detected\n\n    def setup_mmd_detector(self, reference_embeddings: np.ndarray):\n        \"\"\"Setup MMD detector with reference embeddings.\"\"\"\n        self.reference_embeddings = reference_embeddings\n        self.mmd_detector = MMDDrift(\n            X_ref=reference_embeddings,\n            p_val=self.drift_config['mmd_p_val']\n        )\n        print(f\"üî¨ MMD detector initialized with {reference_embeddings.shape[0]} reference samples\")\n\n    def detect_mmd_drift(self, current_embeddings: np.ndarray) -> Dict[str, Any]:\n        \"\"\"Detect drift using MMD test on embeddings.\"\"\"\n        if self.mmd_detector is None:\n            return {'is_drift': False, 'p_value': 1.0, 'distance': 0.0}\n        \n        try:\n            result = self.mmd_detector.predict(current_embeddings)\n            return {\n                'is_drift': bool(result['data']['is_drift']),\n                'p_value': float(result['data']['p_val']),\n                'distance': float(result['data']['distance'])\n            }\n        except Exception as e:\n            print(f\"‚ö†Ô∏è MMD detection failed: {e}\")\n            return {'is_drift': False, 'p_value': 1.0, 'distance': 0.0}\n\n    def detect_evidently_drift(self, reference_data: pd.DataFrame, \n                             current_data: pd.DataFrame) -> Dict[str, Any]:\n        \"\"\"Detect data drift using Evidently.\"\"\"\n        try:\n            # Create drift report\n            report = Report(metrics=[DataDriftPreset()])\n            report.run(reference_data=reference_data, current_data=current_data)\n            \n            # Extract results\n            result_dict = report.as_dict()\n            drift_share = result_dict['metrics'][0]['result']['drift_share']\n            \n            return {\n                'is_drift': drift_share > self.drift_config['evidently_threshold'],\n                'drift_share': drift_share,\n                'drifted_features': result_dict['metrics'][0]['result']['number_of_drifted_columns']\n            }\n        except Exception as e:\n            print(f\"‚ö†Ô∏è Evidently detection failed: {e}\")\n            return {'is_drift': False, 'drift_share': 0.0, 'drifted_features': 0}\n\n\nclass DriftInjector:\n    \"\"\"Handles synthetic drift injection for testing.\"\"\"\n\n    def __init__(self, drift_intensity: float = 0.3):\n        self.drift_intensity = drift_intensity\n        self.setup_augmenters()\n\n    def setup_augmenters(self):\n        \"\"\"Setup text augmentation tools with fallback handling.\"\"\"\n        try:\n            # Try WordNet-based augmentation\n            self.synonym_aug = naw.SynonymAug(aug_src='wordnet', aug_p=self.drift_intensity)\n            self.vocab_drift_available = True\n            print(\"‚úÖ WordNet augmenter initialized for vocabulary drift\")\n        except:\n            # Fallback to simpler augmentation\n            self.synonym_aug = naw.RandomWordAug(action=\"swap\", aug_p=self.drift_intensity)\n            self.vocab_drift_available = False\n            print(\"‚ö†Ô∏è WordNet unavailable, using word swap for vocabulary drift\")\n\n    def inject_label_noise(self, texts: List[str], labels: List[int], \n                          intensity: float = 0.2) -> Tuple[List[str], List[int]]:\n        \"\"\"Inject label noise drift.\"\"\"\n        labels = np.array(labels)\n        num_samples = len(labels)\n        num_to_flip = int(num_samples * intensity)\n        \n        if num_to_flip > 0:\n            # Randomly select indices to flip\n            indices_to_flip = np.random.choice(num_samples, num_to_flip, replace=False)\n            \n            for idx in indices_to_flip:\n                original_label = labels[idx]\n                # Flip to random different label\n                possible_labels = [i for i in range(4) if i != original_label]\n                labels[idx] = np.random.choice(possible_labels)\n        \n        print(f\"üîÑ Label noise: flipped {num_to_flip}/{num_samples} labels\")\n        return texts, labels.tolist()\n\n    def inject_vocab_drift(self, texts: List[str], labels: List[int]) -> Tuple[List[str], List[int]]:\n        \"\"\"Inject vocabulary shift drift.\"\"\"\n        if not self.vocab_drift_available:\n            print(\"‚ö†Ô∏è Vocabulary drift not available, skipping\")\n            return texts, labels\n\n        try:\n            augmented_texts = []\n            for text in texts:\n                try:\n                    aug_text = self.synonym_aug.augment(text)\n                    augmented_texts.append(aug_text[0] if isinstance(aug_text, list) else aug_text)\n                except:\n                    augmented_texts.append(text)  # Keep original if augmentation fails\n            \n            print(f\"üîÑ Vocabulary drift: augmented {len(texts)} texts\")\n            return augmented_texts, labels\n        except Exception as e:\n            print(f\"‚ö†Ô∏è Vocabulary augmentation failed: {e}\")\n            return texts, labels\n\n    def apply_drift(self, dataset: AGNewsDataset, drift_types: List[str]) -> AGNewsDataset:\n        \"\"\"Apply specified drift types to dataset.\"\"\"\n        texts = dataset.texts.copy()\n        labels = dataset.labels.copy()\n\n        for drift_type in drift_types:\n            if drift_type == 'label_noise':\n                texts, labels = self.inject_label_noise(texts, labels, self.drift_intensity)\n            elif drift_type == 'vocab_shift':\n                texts, labels = self.inject_vocab_drift(texts, labels)\n            else:\n                print(f\"‚ö†Ô∏è Unknown drift type: {drift_type}\")\n\n        # Create new drifted dataset\n        return AGNewsDataset(texts, labels, dataset.tokenizer, dataset.max_length)\n\n\nprint(\"‚úÖ Drift detection system ready!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üîç Multi-Level Drift Detection System\n\nComprehensive drift detection using ADWIN, MMD, and Evidently with synthetic drift injection.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class AGNewsDataset(Dataset):\n    \"\"\"Custom PyTorch Dataset for AG News with drift support.\"\"\"\n\n    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = int(self.labels[idx])\n\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n\nclass FederatedDataLoader:\n    \"\"\"Handles federated dataset creation with non-IID partitioning.\"\"\"\n\n    def __init__(self, num_clients: int, alpha: float = 0.5, batch_size: int = 16):\n        self.num_clients = num_clients\n        self.alpha = alpha  # Dirichlet concentration parameter\n        self.batch_size = batch_size\n        self.tokenizer = None\n\n    def create_federated_splits(self, config: Dict[str, Any]):\n        \"\"\"Create federated data splits from AG News dataset.\"\"\"\n        print(\"üì• Loading AG News dataset...\")\n        \n        # Load AG News dataset\n        dataset = load_dataset(\"ag_news\")\n        train_dataset = dataset['train']\n        test_dataset = dataset['test']\n\n        # Create tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(config['model']['model_name'])\n\n        # Extract texts and labels\n        train_texts = train_dataset['text']\n        train_labels = train_dataset['label']\n        test_texts = test_dataset['text']\n        test_labels = test_dataset['label']\n\n        print(f\"üìä Dataset loaded: {len(train_texts)} training, {len(test_texts)} test samples\")\n\n        # Create federated partitions using Dirichlet distribution\n        client_datasets = self._create_dirichlet_splits(train_texts, train_labels, config)\n\n        # Create global test dataset\n        test_dataset_obj = AGNewsDataset(test_texts, test_labels, self.tokenizer, \n                                       config['model']['max_length'])\n\n        return client_datasets, test_dataset_obj\n\n    def _create_dirichlet_splits(self, texts: List[str], labels: List[int], config: Dict[str, Any]):\n        \"\"\"Create non-IID splits using Dirichlet distribution.\"\"\"\n        print(f\"üîÑ Creating non-IID splits with Œ±={self.alpha}...\")\n\n        texts = np.array(texts)\n        labels = np.array(labels)\n        num_classes = config['model']['num_classes']\n\n        # Group samples by class\n        class_indices = [np.where(labels == c)[0] for c in range(num_classes)]\n\n        client_datasets = {}\n\n        for client_id in range(self.num_clients):\n            client_texts = []\n            client_labels = []\n\n            # Sample from Dirichlet distribution for class proportions\n            proportions = np.random.dirichlet(np.repeat(self.alpha, num_classes))\n\n            for class_id in range(num_classes):\n                class_samples = class_indices[class_id]\n                num_samples = int(len(class_samples) * proportions[class_id] / self.num_clients)\n                \n                if num_samples > 0:\n                    selected_indices = np.random.choice(class_samples, num_samples, replace=False)\n                    client_texts.extend(texts[selected_indices])\n                    client_labels.extend(labels[selected_indices])\n\n            # Ensure minimum samples per client\n            min_samples = config['federated']['min_samples_per_client']\n            if len(client_texts) < min_samples:\n                # Add random samples to reach minimum\n                all_indices = np.arange(len(texts))\n                additional_indices = np.random.choice(all_indices, min_samples - len(client_texts), replace=False)\n                client_texts.extend(texts[additional_indices])\n                client_labels.extend(labels[additional_indices])\n\n            # Create dataset for client\n            client_dataset = AGNewsDataset(\n                client_texts, client_labels, self.tokenizer, \n                config['model']['max_length']\n            )\n            client_datasets[client_id] = client_dataset\n\n            print(f\"üë§ Client {client_id}: {len(client_texts)} samples\")\n\n        return client_datasets\n\n\nprint(\"‚úÖ Data handling components ready!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üìä Data Handling and Federated Partitioning\n\nAG News dataset loading, preprocessing, and non-IID partitioning with drift injection capabilities.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class BERTClassifier(nn.Module):\n    \"\"\"BERT-tiny classifier optimized for federated learning.\"\"\"\n\n    def __init__(self, model_name: str, num_classes: int = 4, dropout: float = 0.1):\n        super().__init__()\n        self.model_name = model_name\n        self.num_classes = num_classes\n\n        # Load BERT configuration and model\n        self.config = AutoConfig.from_pretrained(model_name)\n        self.bert = AutoModel.from_pretrained(model_name, config=self.config)\n\n        # Classification head\n        self.dropout = nn.Dropout(dropout)\n        self.classifier = nn.Linear(self.config.hidden_size, num_classes)\n\n        # Initialize classifier weights\n        nn.init.normal_(self.classifier.weight, std=0.02)\n        nn.init.zeros_(self.classifier.bias)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        # BERT forward pass\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n\n        # Use [CLS] token representation\n        pooled_output = outputs.pooler_output\n        pooled_output = self.dropout(pooled_output)\n\n        # Classification\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            loss_fn = nn.CrossEntropyLoss()\n            loss = loss_fn(logits, labels)\n\n        return {'loss': loss, 'logits': logits, 'hidden_states': outputs.last_hidden_state}\n\n    def get_embeddings(self, input_ids, attention_mask):\n        \"\"\"Extract embeddings for drift detection.\"\"\"\n        with torch.no_grad():\n            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n            # Return [CLS] token embeddings\n            return outputs.pooler_output\n\n\ndef create_model_and_tokenizer(config: Dict[str, Any], device: torch.device):\n    \"\"\"Create BERT model and tokenizer.\"\"\"\n    model_name = config['model']['model_name']\n    num_classes = config['model']['num_classes']\n    dropout = config['model']['dropout']\n\n    # Create tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    # Create model\n    model = BERTClassifier(model_name, num_classes, dropout)\n    model = model.to(device)\n\n    # Enable mixed precision if GPU available\n    if device.type == 'cuda':\n        model = model.half()  # Use FP16 for memory efficiency\n\n    return model, tokenizer\n\n\nprint(\"‚úÖ BERT model implementation ready!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ü§ñ BERT Model Implementation\n\nBERT-tiny classifier with GPU optimization for federated learning.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Global Configuration for Federated Learning Drift Detection System\nCONFIG = {\n    # Model configuration\n    'model': {\n        'model_name': 'prajjwal1/bert-tiny',\n        'num_classes': 4,\n        'max_length': 128,\n        'batch_size': 16,\n        'learning_rate': 2e-5,\n        'num_epochs': 3,\n        'warmup_steps': 100,\n        'dropout': 0.1\n    },\n\n    # Federated learning configuration\n    'federated': {\n        'num_clients': 10,\n        'alpha': 0.5,  # Dirichlet concentration for non-IID\n        'min_samples_per_client': 50\n    },\n\n    # Drift configuration\n    'drift': {\n        'injection_round': 25,\n        'drift_intensity': 0.3,\n        'affected_clients': [2, 5, 8],  # Which clients get drift\n        'drift_types': ['label_noise', 'vocab_shift']\n    },\n\n    # Drift detection configuration\n    'drift_detection': {\n        'adwin_delta': 0.002,\n        'mmd_p_val': 0.05,\n        'mmd_permutations': 100,\n        'evidently_threshold': 0.25,\n        'trimmed_beta': 0.2,  # For FedTrimmedAvg\n    },\n\n    # Simulation configuration\n    'simulation': {\n        'num_rounds': 50,\n        'fraction_fit': 1.0,\n        'fraction_evaluate': 1.0,\n        'min_fit_clients': 2,\n        'min_evaluate_clients': 2,\n        'mitigation_threshold': 0.3  # >30% clients reporting drift\n    }\n}\n\nprint(\"‚úÖ Configuration loaded:\")\nprint(f\"üìä Clients: {CONFIG['federated']['num_clients']}\")\nprint(f\"üîÑ Rounds: {CONFIG['simulation']['num_rounds']}\")\nprint(f\"üí• Drift injection: Round {CONFIG['drift']['injection_round']}\")\nprint(f\"üéØ Affected clients: {CONFIG['drift']['affected_clients']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ‚öôÔ∏è Configuration and Constants\n\nDefine all configuration parameters for the federated learning simulation.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "main-title"
   },
   "source": [
    "# üîÑ Federated LLM Drift Detection and Recovery System\n",
    "\n",
    "**A comprehensive standalone implementation for Google Colab with GPU acceleration**\n",
    "\n",
    "## üìã System Overview\n",
    "\n",
    "This notebook implements a sophisticated **multi-level drift detection architecture** for federated learning with BERT-tiny models:\n",
    "\n",
    "### üèóÔ∏è **Architecture Components**\n",
    "- **Client-Side Detection**: ADWIN (concept drift) + Evidently (data drift)\n",
    "- **Server-Side Detection**: MMD statistical test on embedding aggregates  \n",
    "- **Adaptive Mitigation**: FedAvg ‚Üí FedTrimmedAvg when drift detected\n",
    "- **Synthetic Drift Injection**: Vocabulary shift, label noise, distribution shift\n",
    "\n",
    "### üéØ **Key Features**\n",
    "- GPU-optimized for Google Colab (T4/P100/V100)\n",
    "- Real-time drift monitoring with visual analytics\n",
    "- Configurable drift scenarios and client heterogeneity\n",
    "- Production-ready federated learning pipeline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-section"
   },
   "source": [
    "## üöÄ Environment Setup and Dependencies\n",
    "\n",
    "First, let's install all required packages and configure the environment for optimal GPU performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-dependencies"
   },
   "outputs": [],
   "source": [
    "# Install required packages with GPU support\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers[torch]>=4.56.0\n",
    "!pip install -q datasets>=4.0.0\n",
    "!pip install -q scikit-learn>=1.7.0\n",
    "!pip install -q flwr[simulation]>=1.20.0\n",
    "!pip install -q alibi-detect>=0.12.0\n",
    "!pip install -q evidently>=0.7.14\n",
    "!pip install -q river>=0.22.0\n",
    "!pip install -q nlpaug>=1.1.11\n",
    "!pip install -q matplotlib seaborn plotly\n",
    "!pip install -q pandas numpy scipy\n",
    "!pip install -q pyyaml\n",
    "\n",
    "print(\"‚úÖ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu-setup"
   },
   "outputs": [],
   "source": [
    "# Configure GPU and environment\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set environment variables for optimal performance\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# Check GPU availability and configure\n",
    "def setup_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"üéÆ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"üìä GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        \n",
    "        # Optimize memory usage\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "        # Set memory fraction to prevent OOM\n",
    "        if hasattr(torch.cuda, 'set_per_process_memory_fraction'):\n",
    "            torch.cuda.set_per_process_memory_fraction(0.9)\n",
    "            \n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"‚ö†Ô∏è No GPU available, using CPU\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "device = setup_device()\n",
    "print(f\"üîß Device configured: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any, Callable\n",
    "from collections import defaultdict\n",
    "\n",
    "# ML and DL imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Federated Learning\n",
    "import flwr as fl\n",
    "from flwr.simulation import start_simulation\n",
    "from flwr.common import Context, Parameters, Scalar\n",
    "from flwr.server.strategy import FedAvg\n",
    "\n",
    "# Drift Detection\n",
    "from alibi_detect import MMDDrift\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset\n",
    "from river.drift import ADWIN\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ All imports loaded successfully!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}